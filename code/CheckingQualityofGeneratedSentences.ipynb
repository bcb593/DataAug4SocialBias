{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330406e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sim='/Users/bidhanbashyal/MSU/Research/ACL2024_Dataug4socialbias/data/SocialGroups/gender/Generated/4/Same_Social_Group.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f393fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org='/Users/bidhanbashyal/MSU/Research/ACL2024_Dataug4socialbias/data/SocialGroups/gender/gender_output_100.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b1fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_opp='/Users/bidhanbashyal/MSU/Research/ACL2024_Dataug4socialbias/data/SocialGroups/gender/Generated/4/Opposite_Social_Group.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5008dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n",
    "\n",
    "def calculate_average_tokens_and_ngrams(sentences, n):\n",
    "    total_tokens = 0\n",
    "    total_ngrams = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenizing the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "        # Generating n-grams for each token and counting them\n",
    "        for token in tokens:\n",
    "            token_ngrams = list(ngrams(token, n)) if len(token) >= n else []\n",
    "            total_ngrams += len(token_ngrams)\n",
    "\n",
    "    average_tokens = total_tokens / len(sentences) if sentences else 0\n",
    "    return average_tokens, total_ngrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentences = []\n",
    "with open(path_sim, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            same_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d8f2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "639adede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file and storing each line as a sentence\n",
    "org_sentences = []\n",
    "with open(path_org, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            org_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6106314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file and storing each line as a sentence\n",
    "opp_sentences = []\n",
    "with open(path_opp, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            opp_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "841c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 41.55670103092783\n",
      "Total number of 1-grams in the tokens: 20238\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams = calculate_average_tokens_and_ngrams(same_sentences, n=1)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of {n}-grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e7981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 41.45360824742268\n",
      "Total number of 1-grams in the tokens: 20344\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams = calculate_average_tokens_and_ngrams(opp_sentences, n=1)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of {n}-grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10c8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 31.693877551020407\n",
      "Total number of 1-grams in the tokens: 13817\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams = calculate_average_tokens_and_ngrams(org_sentences, n=1)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of {n}-grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f117ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f23d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
