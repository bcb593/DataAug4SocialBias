{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330406e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org='../SentenceGeneration/Data/DebiasingCorpus/CDA/corpus_10-40_20k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f393fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sim='../SentenceGeneration/Data/DebiasingCorpus/Original/corpus_10-40_20k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5008dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bidhanbashyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n",
    "\n",
    "def calculate_total_tokens_and_ngrams(sentences):\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenizing the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "        # Updating the set of unique tokens\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    total_unique_tokens = len(unique_tokens)\n",
    "\n",
    "    return total_tokens/len(sentences), total_unique_tokens/len(sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4d3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentences = []\n",
    "with open(path_sim, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            same_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "639adede",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences = []\n",
    "with open(path_org, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            org_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f947f431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the trump administration’s red line was apparently crossed this month, when a white house spokesman issued a rare rebuke of netanyahu, dismissing reports that us officials had discussed an annexation plan for the west bank with their israeli counterparts.',\n",
       " 'the trump administration ’ s red line was apparently crossed this month , when a white house spokeswoman issued a rare rebuke of netanyahu , dismissing reports that us officials had discussed an annexation plan for the west bank with their israeli counterparts .']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcadc3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The trump administration's red line was crossed when a white house spokesman issued a rare rebuke of netanyahu, dismissing reports of annexation plan for the west bank with their israeli counterparts, and this move was seen as a significant escalation in the ongoing tensions between the two nations.\",\n",
       " \"The trump administration 's red line was crossed when a white house spokeswoman issued a rare rebuke of netanyahu , dismissing reports of annexation plan for the west bank with their israeli counterparts , and this move was seen as a significant escalation in the ongoing tensions between the two nations .\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde2f7c",
   "metadata": {},
   "source": [
    "## Checking number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "841c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 28.362\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(same_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10c8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 52.6546\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(org_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f117ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(same_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e70d4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc0d62",
   "metadata": {},
   "source": [
    "## Checking similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f23d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity: 0.3234522517869306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "l=[]\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "total_similarity = 0\n",
    "for i in range(min(len(org_sentences), len(same_sentences))):\n",
    "    tfidf_matrix = vectorizer.fit_transform([org_sentences[i], same_sentences[i]])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    #print(f\"Similarity of sentence {i+1}: {similarity[0][0]}\")\n",
    "    total_similarity += similarity[0][0]\n",
    "\n",
    "average_similarity = total_similarity / min(len(org_sentences), len(same_sentences))\n",
    "print(f\"Average Similarity: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79138cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd99b8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Moreover, this flawed mindset overlooks the fact that a marriage should be built on mutual respect, trust, and equality, rather than on the idea that one partner has the right to control or dominate the other. By recognizing and challenging such harmful beliefs, we can work towards creating a safer and more supportive environment for all individuals in abusive relationships.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sentences[9454]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153d50f",
   "metadata": {},
   "source": [
    "she claims she will not stop until your family is dead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a80c88",
   "metadata": {},
   "source": [
    "## Checking text token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6edc92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "    print(len(sentences_list))\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of types (unique words)\n",
    "    types = len(set(tokens))\n",
    "\n",
    "    # Calculate the number of tokens (total words)\n",
    "    tokens_total = len(tokens)\n",
    "\n",
    "    # Calculate the type-token ratio (TTR)\n",
    "    ttr = types / tokens_total if tokens_total > 0 else 0\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9229605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "Text-to-token ratio for list1: 0.13\n",
      "Text-to-token ratio for list2: 0.06\n"
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_ttr(org_sentences)\n",
    "ratio_list2 = calculate_ttr(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eccaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hapax_legomenon_ratio(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of hapax legomena (words occurring only once)\n",
    "    hapax_legomena = [word for word in set(tokens) if tokens.count(word) == 1]\n",
    "\n",
    "    # Calculate the hapax legomenon ratio (HLR)\n",
    "    hlr = len(hapax_legomena) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    return hlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c22cfa49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratio_list1 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_hapax_legomenon_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43morg_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ratio_list2 \u001b[38;5;241m=\u001b[39m calculate_hapax_legomenon_ratio(same_sentences)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText-to-token ratio for list1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratio_list1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36mcalculate_hapax_legomenon_ratio\u001b[0;34m(sentences_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m get_word_tokens(sentences_list)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the number of hapax legomena (words occurring only once)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m hapax_legomena \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tokens) \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mcount(word) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the hapax legomenon ratio (HLR)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m hlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hapax_legomena) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m get_word_tokens(sentences_list)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the number of hapax legomena (words occurring only once)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m hapax_legomena \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tokens) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the hapax legomenon ratio (HLR)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m hlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hapax_legomena) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_hapax_legomenon_ratio(org_sentences)\n",
    "ratio_list2 = calculate_hapax_legomenon_ratio(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca507424",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c968b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability_scores(sentences):\n",
    "    # Join the list of sentences into a single text\n",
    "    text = ' '.join(sentences)\n",
    "\n",
    "    # Calculate readability scores\n",
    "    flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "\n",
    "    return flesch_kincaid, gunning_fog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63775130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 23.4\n",
      "Gunning Fog Index: 18.82\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(same_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d4f2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 12.5\n",
      "Gunning Fog Index: 10.44\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(org_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51ecef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
