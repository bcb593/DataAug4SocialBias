{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330406e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org='../SentenceGeneration/Data/DebiasingCorpus/CDA/corpus_10-40_20k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f393fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sim='../SentenceGeneration/Data/DebiasingCorpus/Original/corpus_10-40_20k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5008dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n",
    "\n",
    "def calculate_total_tokens_and_ngrams(sentences):\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenizing the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "        # Updating the set of unique tokens\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    total_unique_tokens = len(unique_tokens)\n",
    "\n",
    "    return total_tokens/len(sentences), total_unique_tokens/len(sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentences = []\n",
    "with open(path_sim, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            same_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639adede",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences = []\n",
    "with open(path_org, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            org_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f947f431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the trump administration’s red line was apparently crossed this month, when a white house spokesman issued a rare rebuke of netanyahu, dismissing reports that us officials had discussed an annexation plan for the west bank with their israeli counterparts.',\n",
       " 'the trump administration ’ s red line was apparently crossed this month , when a white house spokeswoman issued a rare rebuke of netanyahu , dismissing reports that us officials had discussed an annexation plan for the west bank with their israeli counterparts .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcadc3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The trump administration's red line was crossed when a white house spokesman issued a rare rebuke of netanyahu, dismissing reports of annexation plan for the west bank with their israeli counterparts, and this move was seen as a significant escalation in the ongoing tensions between the two nations.\",\n",
       " \"The trump administration 's red line was crossed when a white house spokeswoman issued a rare rebuke of netanyahu , dismissing reports of annexation plan for the west bank with their israeli counterparts , and this move was seen as a significant escalation in the ongoing tensions between the two nations .\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde2f7c",
   "metadata": {},
   "source": [
    "## Checking number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 28.362\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(same_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 52.6546\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(org_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f117ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(same_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e70d4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc0d62",
   "metadata": {},
   "source": [
    "## Checking similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81f23d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity: 0.3234522517869306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "l=[]\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "total_similarity = 0\n",
    "for i in range(min(len(org_sentences), len(same_sentences))):\n",
    "    tfidf_matrix = vectorizer.fit_transform([org_sentences[i], same_sentences[i]])\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    #print(f\"Similarity of sentence {i+1}: {similarity[0][0]}\")\n",
    "    total_similarity += similarity[0][0]\n",
    "\n",
    "average_similarity = total_similarity / min(len(org_sentences), len(same_sentences))\n",
    "print(f\"Average Similarity: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a80c88",
   "metadata": {},
   "source": [
    "## Checking text token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aba22306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(sentences_list):\n",
    "    ttr_per_sentence = []\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        tokens = sentence.split()\n",
    "        types = len(set(tokens))\n",
    "        tokens_total = len(tokens)\n",
    "        ttr = types / tokens_total if tokens_total > 0 else 0\n",
    "        ttr_per_sentence.append(ttr)\n",
    "    average_ttr = sum(ttr_per_sentence) / len(ttr_per_sentence) if len(ttr_per_sentence) > 0 else 0\n",
    "\n",
    "    return ttr_per_sentence, average_ttr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9229605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-token ratio for list1(Original Sentence): 0.81\n",
      "Text-to-token ratio for list2(generated Sentence): 0.91\n"
     ]
    }
   ],
   "source": [
    "_,ratio_list1 = calculate_ttr(org_sentences)\n",
    "_,ratio_list2 = calculate_ttr(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1(Original Sentence): {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2(generated Sentence): {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hapax_legomenon_ratio(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of hapax legomena (words occurring only once)\n",
    "    hapax_legomena = [word for word in set(tokens) if tokens.count(word) == 1]\n",
    "\n",
    "    # Calculate the hapax legomenon ratio (HLR)\n",
    "    hlr = len(hapax_legomena) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    return hlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cfa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list1 = calculate_hapax_legomenon_ratio(org_sentences)\n",
    "ratio_list2 = calculate_hapax_legomenon_ratio(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca507424",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c968b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability_scores(sentences):\n",
    "    # Join the list of sentences into a single text\n",
    "    text = ' '.join(sentences)\n",
    "\n",
    "    # Calculate readability scores\n",
    "    flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "\n",
    "    return flesch_kincaid, gunning_fog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63775130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 23.4\n",
      "Gunning Fog Index: 18.82\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(same_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d4f2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 12.5\n",
      "Gunning Fog Index: 10.44\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(org_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389e7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
