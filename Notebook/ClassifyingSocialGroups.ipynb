{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f44b94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import re\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0193e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0478866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(input_file, output_file, keywords):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content into sentences\n",
    "    sentences = tokenizer.tokenize(content)\n",
    "\n",
    "    # Prepare the regex pattern for whole word match\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(keyword) for keyword in keywords) + r')\\b'\n",
    "\n",
    "    # Filter sentences containing any of the keywords as whole words\n",
    "    filtered_sentences = [sentence for sentence in sentences if re.search(pattern, sentence, re.IGNORECASE)]\n",
    "\n",
    "    # Write the filtered sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for sentence in filtered_sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8ce722",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/SentenceGeneration/Data/gender_wordlist/man_word_list.txt'\n",
    "file2='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/SentenceGeneration/Data/gender_wordlist/woman_word_list.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1616cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        keywords = [line.strip() for line in file.readlines()]\n",
    "    return keywords\n",
    "\n",
    "# Load gender keywords from two files\n",
    "file1_keywords = load_keywords_from_file(file1)\n",
    "file2_keywords = load_keywords_from_file(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1468c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_keywords =file1_keywords + file2_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b353011",
   "metadata": {},
   "outputs": [],
   "source": [
    "se=extract_sentences('/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/Sanitycheck/Data/Debiasing_data/debiasing.train.txt', 'gender_output.txt', gender_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d56bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_save_sentences(file_path, output_file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Splitting the text into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    # Removing extra spaces and line breaks within sentences\n",
    "    sentences = [sentence.strip().replace('\\n', ' ').replace('\\r', '') for sentence in sentences]\n",
    "\n",
    "    # Writing cleaned sentences to a new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for sentence in sentences:\n",
    "            output_file.write(sentence + '\\n')\n",
    "\n",
    "    return len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b21965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned sentences: 94265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace these with your actual file paths\n",
    "file_path = 'gender_output.txt'\n",
    "output_file_path = 'gender_output_normalized.txt'\n",
    "\n",
    "sentence_count = clean_and_save_sentences(file_path, output_file_path)\n",
    "print(f\"Number of cleaned sentences: {sentence_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7773efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 90655\n",
      "Maximum sentence length: 343\n",
      "Average sentence length: 28.62\n",
      "Mode of sentence length: 24\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_stats(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content into sentences using the initialized tokenizer\n",
    "    sentences = tokenizer.tokenize(content)\n",
    "\n",
    "    # Calculate sentence lengths\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_sentences = len(sentences)\n",
    "    max_sentence_length = max(sentence_lengths)\n",
    "    average_sentence_length = sum(sentence_lengths) / total_sentences\n",
    "    mode_sentence_length = mode(sentence_lengths)\n",
    "\n",
    "    return total_sentences, max_sentence_length, average_sentence_length, mode_sentence_length\n",
    "\n",
    "# Replace 'your_corpus.txt' with the actual path to your corpus file\n",
    "corpus_stats = calculate_sentence_stats('gender_output_normalized.txt')\n",
    "\n",
    "print(f\"Total sentences: {corpus_stats[0]}\")\n",
    "print(f\"Maximum sentence length: {corpus_stats[1]}\")\n",
    "print(f\"Average sentence length: {corpus_stats[2]:.2f}\")\n",
    "print(f\"Mode of sentence length: {corpus_stats[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8006b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_save_sentences_by_length(corpus_path, min_length, max_length, output_file):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.readlines()\n",
    "\n",
    "    filtered_sentences = [sentence.strip() for sentence in corpus if min_length <= len(sentence.split()) <= max_length]\n",
    "\n",
    "    # Save the filtered sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for sentence in filtered_sentences:\n",
    "            file.write(sentence + '\\n')\n",
    "\n",
    "    return len(filtered_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da697692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered sentences: 14905\n"
     ]
    }
   ],
   "source": [
    "filtered_sentences_count = filter_and_save_sentences_by_length('../SentenceGeneration/Data/DebiasingCorpus/gender_output_normalized.txt', 40,100,'gender(40-100).txt')\n",
    "print(f\"Number of filtered sentences: {filtered_sentences_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9f8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_sentences(corpus_path, sample_size, output_file):\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.readlines()\n",
    "\n",
    "    # Ensure that the sample size is not greater than the total number of sentences\n",
    "    sample_size = min(sample_size, len(corpus))\n",
    "\n",
    "    # Randomly select sample_size sentences\n",
    "    random_sentences = random.sample(corpus, sample_size)\n",
    "\n",
    "    # Save the randomly selected sentences to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for sentence in random_sentences:\n",
    "            file.write(sentence)\n",
    "\n",
    "    return random_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bedea7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of randomly selected sentences: 10000\n"
     ]
    }
   ],
   "source": [
    "corpus_path = '../SentenceGeneration/Data/DebiasingCorpus/gender(40-100).txt'  # Replace with the actual path to your corpus file\n",
    "sample_size = 10000\n",
    "output_file = 'corpus(40-100)10k.txt'\n",
    "\n",
    "random_sentences = get_random_sentences(corpus_path, sample_size, output_file)\n",
    "print(f\"Number of randomly selected sentences: {len(random_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b05cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
