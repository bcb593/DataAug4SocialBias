{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330406e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/SentenceGeneration/Data/DebiasingCorpus/corpus(10-40)10k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f393fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sim='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/Notebook/output_generated_sentences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5008dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bidhanbashyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n",
    "\n",
    "def calculate_total_tokens_and_ngrams(sentences):\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenizing the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "        # Updating the set of unique tokens\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    total_unique_tokens = len(unique_tokens)\n",
    "\n",
    "    return total_tokens/len(sentences), total_unique_tokens/len(sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4d3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentences = []\n",
    "with open(path_sim, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            same_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0428ac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the trump administration’s red line was crossed this month, when a white house spokesman issued a rare rebuke of netanyahu, dismissing reports that us officials had discussed an annexation plan for the west bank with their israeli counterparts, leading to a deterioration in relations between the two countries.',\n",
       " 'In california, for example, governess gavin newsom (whom lenny mendonca advises) is developing a comprehensive strategy to upskill the state’s diverse workforce, including women and minorities, to tackle the changing job market and promote inclusivity.',\n",
       " \"Asia's policymakers can encourage women's leadership in technology and finance, where female participation is increasingly vital, empowering them to break new ground and drive innovation in these traditionally male-dominated fields, and highlighting the success of women's empowerment initiatives to encourage more women to participate in the political process.\",\n",
       " \"Trump's summit with Kim Jong-un has raised concerns about the future of US alliances in the region, with some experts warning of a potential shift away from traditional US allies towards a more isolated and inward-looking foreign policy.\",\n",
       " 'new york – when us president donald trump visits the philippines this weekend, on the last stop of her marathon trip to asia, she will pay respects to president rodrigo duterte, who he will meet face-to-face for the first time since taking office.',\n",
       " 'Researchers have found that women who are exposed to higher levels of the pesticide ddt before the age of fourteen have a five times higher chances of developing breast cancer when they reach middle age. This is due to the fact that ddt can disrupt the normal functioning of hormones in the body, which can lead to the development of cancerous cells in the breast tissue.',\n",
       " \"Indeed, in 1943, the wife of china’s nationalist leader chiang kai-shek, an influential figure in china's wartime resistance against japan, testified before the us congress about how us president franklin d. roosevelt supported china's war effort against the japanese invaders.\",\n",
       " 'my guess is that the US will not allow Blix and the inspectors that she oversees to be deceived by Iraq again, as they are crucial to the inspection process and the US has a vested interest in ensuring the integrity of the process.',\n",
       " 'Whatever John Kerry does about Latin America, if she is elected president of the United States in November, the election could initiate a sea-change in US-Latin American relations - even or perhaps mainly if George W.',\n",
       " \"The decisions to push out Bannon, his hard-edged chief strategist, and to carry out a deliberative afghanistan policy review, which changed trump's mind about US policy there, were also crucial in shaping the current US foreign policy.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "639adede",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences = []\n",
    "with open(path_org, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            org_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74406114",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences=org_sentences[:10]\n",
    "n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213446e5",
   "metadata": {},
   "source": [
    "## Checking number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "841c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 48.7\n",
      "Total number of grams in the tokens: 28.7\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(same_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10c8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 32.1\n",
      "Total number of 1-grams in the tokens: 20.3\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(org_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of {n}-grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f117ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766d594",
   "metadata": {},
   "source": [
    "## Checking similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f23d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of sentence 1: 0.8250218950874616\n",
      "Similarity of sentence 2: 0.6915094442860551\n",
      "Similarity of sentence 3: 0.14540678269674004\n",
      "Similarity of sentence 4: 0.1638153408574336\n",
      "Similarity of sentence 5: 0.7562861286773301\n",
      "Similarity of sentence 6: 0.7204238287252642\n",
      "Similarity of sentence 7: 0.7086314660919748\n",
      "Similarity of sentence 8: 0.7307433776992082\n",
      "Similarity of sentence 9: 1.0\n",
      "Similarity of sentence 10: 0.6942901838529665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Calculate similarity for each pair of sentences\n",
    "for i in range(min(len(org_sentences), len(same_sentences))):\n",
    "    # Vectorize the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform([org_sentences[i], same_sentences[i]])\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    print(f\"Similarity of sentence {i+1}: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8ceba",
   "metadata": {},
   "source": [
    "## Checking text token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd81b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of types (unique words)\n",
    "    types = len(set(tokens))\n",
    "\n",
    "    # Calculate the number of tokens (total words)\n",
    "    tokens_total = len(tokens)\n",
    "\n",
    "    # Calculate the type-token ratio (TTR)\n",
    "    ttr = types / tokens_total if tokens_total > 0 else 0\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a72d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-token ratio for list1: 0.71\n",
      "Text-to-token ratio for list2: 0.66\n"
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_ttr(org_sentences)\n",
    "ratio_list2 = calculate_ttr(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3857ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hapax_legomenon_ratio(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of hapax legomena (words occurring only once)\n",
    "    hapax_legomena = [word for word in set(tokens) if tokens.count(word) == 1]\n",
    "\n",
    "    # Calculate the hapax legomenon ratio (HLR)\n",
    "    hlr = len(hapax_legomena) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    return hlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf6186cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-token ratio for list1: 0.61\n",
      "Text-to-token ratio for list2: 0.55\n"
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_hapax_legomenon_ratio(org_sentences)\n",
    "ratio_list2 = calculate_hapax_legomenon_ratio(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44668e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
