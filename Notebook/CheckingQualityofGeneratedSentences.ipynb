{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "330406e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/SentenceGeneration/Data/DebiasingCorpus/corpus(10-40)10k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f393fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sim='/Users/bidhanbashyal/MSU/Research/DataAug4SocialBias/Notebook/output_generated_sentences_10_better.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a5008dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bidhanbashyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n",
    "\n",
    "def calculate_total_tokens_and_ngrams(sentences):\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenizing the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "        # Updating the set of unique tokens\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    total_unique_tokens = len(unique_tokens)\n",
    "\n",
    "    return total_tokens/len(sentences), total_unique_tokens/len(sentences)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d4d3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sentences = []\n",
    "with open(path_sim, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            same_sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3b1f900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As the US and Israel navigate their intricate relationship, the recent incident involving the White House spokesman and Netanyahu has introduced a new layer of complexity, prompting both nations to reassess their stance',\n",
       " 'With the visionary leadership of Governor Gavin Newsom and the guidance of Lenny Mendonca, California is embarking on a groundbreaking initiative to upskill its diverse workforce, empowering them to seize the opportunities presented by the rapidly evolving technological landscape, thereby fostering a future',\n",
       " 'Despite the scarcity of information about workshops for womenâ€™s empowerment and the successful organization of small-scale local elections across the country, there is a pressing need to bridge this knowledge gap and provide a comprehensive understanding of these initiatives, enabling a more informed and inclusive decision-making process.',\n",
       " \"Moreover, the looming summit between Trump and Kim casts a shadow of uncertainty over the fate of the US-Korea alliance, as Trump's unpredictable approach to diplomacy has already strained the longstanding partnership between the two nations, potentially paving the way for a drastic transformation in the geopolitical landscape of the region.\",\n",
       " \"In addition to the official visit of the US president, the trip also includes a visit to the war-torn Marawi City, where the president will pay respects to the troops and civilians who have lost their lives in the conflict, demonstrating the US's commitment to the region's peace and stability.\",\n",
       " 'Furthermore, researchers have found that early exposure to DDT, particularly during critical developmental periods, can have long-lasting effects on breast development and increase the risk of breast cancer later in life, highlighting the need for stricter regulations and public awareness campaigns to minimize exposure to this harmful pesticide.',\n",
       " \"Furthermore, Chiang Kai-shek's wife was a prominent advocate for women's rights and education, actively promoting gender equality and empowering women in china.\",\n",
       " \"In light of the US's steadfast commitment to ensuring the integrity of the inspection process, the speaker surmises that the US will not stand idly by while Blix and the inspectors are being misled by Iraq, instead, the US will exert its influence to prevent any further deception and\",\n",
       " \"As the election of a female president in the United States in November could herald a transformative shift in US-Latin American relations, John Kerry's policies towards the region take on added significance, with his leadership potentially ushering in a new era of co\",\n",
       " 'As the world grapples with the complexities of korean unification and global peace, the thoughtful deliberation and strategic decision-making demonstrated in the #Original Sentence# serve']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "639adede",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences = []\n",
    "with open(path_org, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Strip to remove any leading/trailing whitespace\n",
    "        sentence = line.strip()\n",
    "        # Only add non-empty sentences\n",
    "        if sentence:\n",
    "            org_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "715bbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_sentences=org_sentences[:10]\n",
    "n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde2f7c",
   "metadata": {},
   "source": [
    "## Checking number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "841c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 45.2\n",
      "Total number of grams in the tokens: 26.1\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(same_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10c8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per sentence: 32.1\n",
      "Total number of 1-grams in the tokens: 20.3\n"
     ]
    }
   ],
   "source": [
    "average_tokens, total_bigrams =  calculate_total_tokens_and_ngrams(org_sentences)\n",
    "print(f\"Average number of tokens per sentence: {average_tokens}\")\n",
    "print(f\"Total number of {n}-grams in the tokens: {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7f117ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc0d62",
   "metadata": {},
   "source": [
    "## Checking similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81f23d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of sentence 1: 0.21418440628898394\n",
      "Similarity of sentence 2: 0.2945828772607368\n",
      "Similarity of sentence 3: 0.45855938512288874\n",
      "Similarity of sentence 4: 0.07495311385871599\n",
      "Similarity of sentence 5: 0.37985104096812444\n",
      "Similarity of sentence 6: 0.21433422603336758\n",
      "Similarity of sentence 7: 0.1318488807660832\n",
      "Similarity of sentence 8: 0.45358262867878574\n",
      "Similarity of sentence 9: 0.412034947426482\n",
      "Similarity of sentence 10: 0.18205213126219905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Calculate similarity for each pair of sentences\n",
    "for i in range(min(len(org_sentences), len(same_sentences))):\n",
    "    # Vectorize the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform([org_sentences[i], same_sentences[i]])\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    print(f\"Similarity of sentence {i+1}: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a80c88",
   "metadata": {},
   "source": [
    "## Checking text token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6edc92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of types (unique words)\n",
    "    types = len(set(tokens))\n",
    "\n",
    "    # Calculate the number of tokens (total words)\n",
    "    tokens_total = len(tokens)\n",
    "\n",
    "    # Calculate the type-token ratio (TTR)\n",
    "    ttr = types / tokens_total if tokens_total > 0 else 0\n",
    "\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9229605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-token ratio for list1: 0.71\n",
      "Text-to-token ratio for list2: 0.65\n"
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_ttr(org_sentences)\n",
    "ratio_list2 = calculate_ttr(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3eccaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hapax_legomenon_ratio(sentences_list):\n",
    "    def get_word_tokens(sentences):\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            # Split each sentence into words\n",
    "            words = sentence.split()\n",
    "            tokens.extend(words)\n",
    "        return tokens\n",
    "\n",
    "    # Get word tokens from the given list\n",
    "    tokens = get_word_tokens(sentences_list)\n",
    "\n",
    "    # Calculate the number of hapax legomena (words occurring only once)\n",
    "    hapax_legomena = [word for word in set(tokens) if tokens.count(word) == 1]\n",
    "\n",
    "    # Calculate the hapax legomenon ratio (HLR)\n",
    "    hlr = len(hapax_legomena) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    return hlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c22cfa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-token ratio for list1: 0.61\n",
      "Text-to-token ratio for list2: 0.56\n"
     ]
    }
   ],
   "source": [
    "ratio_list1 = calculate_hapax_legomenon_ratio(org_sentences)\n",
    "ratio_list2 = calculate_hapax_legomenon_ratio(same_sentences)\n",
    "\n",
    "print(f\"Text-to-token ratio for list1: {ratio_list1:.2f}\")\n",
    "print(f\"Text-to-token ratio for list2: {ratio_list2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca507424",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c968b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability_scores(sentences):\n",
    "    # Join the list of sentences into a single text\n",
    "    text = ' '.join(sentences)\n",
    "\n",
    "    # Calculate readability scores\n",
    "    flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "\n",
    "    return flesch_kincaid, gunning_fog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "63775130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 31.2\n",
      "Gunning Fog Index: 34.89\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(same_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9d4f2d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 12.9\n",
      "Gunning Fog Index: 15.56\n"
     ]
    }
   ],
   "source": [
    "flesch_kincaid_score, gunning_fog_score = calculate_readability_scores(org_sentences)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_score}\")\n",
    "print(f\"Gunning Fog Index: {gunning_fog_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45368e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
